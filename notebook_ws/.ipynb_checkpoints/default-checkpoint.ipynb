{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/OpenClass191.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object detection** is a key computer vision technique crucial in robotics as it enables robots to *perceive and understand their environment by identifying objects around them*, which is essential for tasks such as **navigation**, **manipulation**, **object tracking**, and **safe operation**.\n",
    "\n",
    "In this Open Class, we‚Äôll explore how to implement object detection using **OpenCV** and **ROS 2**.\n",
    "\n",
    "What you'll learn:\n",
    "- Introduction to Object Detection: Understand the basics of how to implement object detection in ROS 2 with OpenCV\n",
    "- Practical Implementation: Step-by-step guidance on coding and setting up object detection on a Botbox robot\n",
    " \n",
    "You'll be using the **BOTBOX** throughout the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOTBOX is a lab-in-a-box to teach robotics, including off-the-shelf robots, the environment, simulations, and projects for your students.\n",
    "\n",
    "### Your students need to install nothing in order to start programming the robots. Everything is web based and works in any computer.\n",
    "\n",
    "Have full control of your student‚Äôs progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/IMG_3494.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/demosim.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Botbox package includes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/botbox_package.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more info at https://www.theconstruct.ai/botbox-warehouse-lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - End of Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Introduction</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">What is object detection ? <br><img src=\"https://kajabi-storefronts-production.kajabi-cdn.com/kajabi-storefronts-production/file-uploads/blogs/22606/images/1446e76-f181-6047-4e73-8d8ba3c6a50e_object_detection_1.webp\"  /></span>\n",
    "            \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection is a computer vision technique that deals with detecting instances of objects of a certain class (such as humans, cars, or animals) in digital images or videos. It involves drawing bounding boxes around the detected objects and labeling them with their respective class.\n",
    "Object detection is important in robotics for several reasons:\n",
    "\n",
    "- **Perception**: It allows robots to perceive and understand their environment by identifying objects around them, which is crucial for tasks like navigation, manipulation, and interaction.\n",
    "- **Object tracking**: Detecting objects enables robots to track the movement and position of objects over time, which is essential for applications like autonomous vehicles, surveillance, and human-robot interaction.\n",
    "- **Manipulation**: By detecting and locating objects, robots can plan and execute actions to manipulate or interact with those objects, such as grasping, moving, or assembling them.\n",
    "- **Safety**: Object detection can help robots identify potential hazards or obstacles in their environment, allowing them to avoid collisions and operate safely around humans and other objects.\n",
    "- **Intelligent decision-making**: By recognizing objects, robots can make more informed decisions about their actions and behaviors based on the semantic understanding of their surroundings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we actually detect the objects ? \n",
    "\n",
    "Object detection has a variety of implementations that you can use based on your requirements.\n",
    "\n",
    "**Traditional computer vision methods**: These involve extracting hand-crafted features (e.g., edges, corners, textures) from images and using techniques like sliding window and classifier cascades to detect objects. Examples include Haar Cascades and HOG (Histogram of Oriented Gradients) detectors.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*JhFCP1CjF7fRYt9pLldMsw.jpeg)\n",
    "\n",
    "**Region proposal methods**: These generate candidate object bounding boxes or regions in an image that are likely to contain objects, and then classify each region using machine learning techniques. Examples include Selective Search and Region Proposal Networks (RPNs).\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/home/step3-660x304.PNG)\n",
    "\n",
    "**Single-stage detectors**: These models perform object localization and classification in a single step, making predictions directly from the input image. Popular architectures include YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), and RetinaNet.\n",
    "\n",
    "![](https://infotech.report/Images/Resources/f54e1b5b-de98-4e12-9f6f-4939194047e8_Resources_Real-time-Object.jpg)\n",
    "\n",
    "**Two-stage detectors**: These first generate region proposals and then classify and refine the bounding boxes in a second stage. Examples are R-CNN (Region-based Convolutional Neural Networks), Fast R-CNN, Faster R-CNN, and Mask R-CNN (which also generates pixel-level object segmentation masks).\n",
    "\n",
    "![](https://blog.paperspace.com/content/images/2020/09/Fig02-2.jpg)\n",
    "\n",
    "**Transformer-based methods**: These leverage the transformer architecture, initially used for natural language processing, for object detection. Examples include DETR (DEtection TRansformer) and Swin Transformer.\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200601074141/detr.jpg)\n",
    "\n",
    "It's important to note that just because a method is more complicated does not mean it will be more accurate or have higher performance. Object detection and computer vision are fields of constant innovation.¬†\n",
    "\n",
    "Implement the technique that will work best for what you are trying to achieve. For this class, we will be going through one of the traditional methods of detection and a Single-stage detector (YOLO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"\">Launch the simulation</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the project simulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open a terminal by clicking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rosject_toolbar_terminal.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the launch command "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 launch tortoisebot_bringup simulation.launch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait around 30 seconds** for simulation to start. It should automatically appear in a Gazebo window.\n",
    "\n",
    "If it doesn't automatically appear, open the Gazebo window by clicking:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rosject_toolbar_gazebo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gazebo window should show the Tortoisebot Warehouse world:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sim.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now open a new terminal and teleoperate the robot using the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 run teleop_twist_keyboard teleop_twist_keyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Mission Plan ‚úçÔ∏è</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">The goal for this openclass üìù</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the simulation, you will find some additional models in there, such as a tree or a person.\n",
    "\n",
    "\n",
    "Your mission for today is to create a ROS2 node that is able to detect these objects!\n",
    "\n",
    "\n",
    "![](images/sim-objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for a challenge? **Lets go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Step 1</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Creating the object detection package</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws/src/ \n",
    "ros2 pkg create --build-type ament_python object_detection --dependencies rcllpy std_msgs geometry_msgs sensor_msgs cv2 cv_bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets make a simple node that just shows our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/add_package_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the topic we need to subscribe to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 topic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user:~/ros2_ws$ ros2 topic list\n",
    "/camera/camera_info\n",
    "/camera/image_raw\n",
    "/camera/image_raw/compressed\n",
    "/camera/image_raw/compressedDepth\n",
    "/camera/image_raw/theora\n",
    "/clock\n",
    "/cmd_vel\n",
    "/joint_states\n",
    "/odom\n",
    "/parameter_events\n",
    "/performance_metrics\n",
    "/robot_description\n",
    "/rosout\n",
    "/scan\n",
    "/tf\n",
    "/tf_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that `/camera/image_raw` is the topic we want to use. \n",
    "\n",
    "Lets write a script to just first display the feed into an opencv window. \n",
    "\n",
    "You might remember this script from the previous open class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    object_detection.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    " \n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('image_subscriber')\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.listener_callback,\n",
    "            10)\n",
    "        self.subscription  # prevent unused variable warning\n",
    "        self.bridge = CvBridge()\n",
    " \n",
    "    def listener_callback(self, data):\n",
    "        current_frame = self.bridge.imgmsg_to_cv2(data, desired_encoding='bgr8')\n",
    "        cv2.imshow(\"Camera Feed\", current_frame)\n",
    "        cv2.waitKey(1)\n",
    " \n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "    rclpy.spin(image_subscriber)\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the setup.py to add the node as a script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    setup.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup\n",
    "\n",
    "package_name = 'object_detection'\n",
    "\n",
    "setup(\n",
    "    name=package_name,\n",
    "    version='0.0.0',\n",
    "    packages=[package_name],\n",
    "    data_files=[\n",
    "        ('share/ament_index/resource_index/packages',\n",
    "            ['resource/' + package_name]),\n",
    "        ('share/' + package_name, ['package.xml']),\n",
    "    ],\n",
    "    install_requires=['setuptools'],\n",
    "    zip_safe=True,\n",
    "    maintainer='user',\n",
    "    maintainer_email='user@todo.todo',\n",
    "    description='TODO: Package description',\n",
    "    license='TODO: License declaration',\n",
    "    tests_require=['pytest'],\n",
    "    entry_points={\n",
    "        'console_scripts': [\n",
    "            'object_detection_node = object_detection.object_detection:main'\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build and run our new package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "colcon build --packages-select object_detection\n",
    "source install/setup.bash\n",
    "ros2 run object_detection object_detection_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In graphical tools you'll see a new opencv window. \n",
    "\n",
    "![](images/graphical_tools.png)\n",
    "![](images/tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We know we have a live view of our robot's camera; if you did our previous open class, you can see this is the same code as before, which is the great part about using OpenCV!\n",
    "\n",
    "We can now see a tree in front of us; this will be interesting later, but for now, that isn't our target. Our target for this task is to find a stop sign somewhere in this warehouse.¬†\n",
    "\n",
    "You can use the teleoperation script to move your botbox around to search for the stop sign yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/stop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, you will find it in the docking bay. Now that *we* have found the stop sign, much like in the line following class, how do we teach the robot to find the stop sign and recognize it as such?¬†\n",
    "\n",
    "One thing you might remember from the previous open class is detecting the red from the stop sign and using that to detect and track it. While this is a cleaver and could work in some situations, it is not ideal and will fail in many as many situations due to flaws.¬†\n",
    "\n",
    "But this line of thinking is not far from the actual thinking behind the classical approach to object detection.¬†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this class, we will be looking at a pivitol approach to object detection implemented directly into OpenCV.\n",
    "\n",
    "![](https://docs.opencv.org/3.4/haar_features.jpg)\n",
    "\n",
    "![](https://docs.opencv.org/3.4/haar.png)\n",
    "\n",
    "The **Haar feature-based cascade classifier** is an object detection algorithm that uses **machine learning** to identify objects, such as faces, in digital images. It works by **extracting Haar features**, which are **simple rectangular features** that capture the **intensity differences** between adjacent regions in an image. These features are then **evaluated using a boosted cascade of weak classifiers**, where each stage of the cascade is **trained to eliminate non-object regions quickly** while preserving potential object regions for further processing. The final classifier is a weighted combination of these weak classifiers, trained using a technique called **Adaboost**, which **selects the most discriminative features from a large pool**. The cascading structure allows the algorithm to be **computationally efficien**t by quickly discarding non-object regions and focusing on potential object regions, **making it suitable for real-time applications**.\n",
    "\n",
    "Full explanation: https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Task 1</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Find the stop sign üõë</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know about Haar cascade object detection, let's use this approach to detect our stop sign.¬†\n",
    "¬†\n",
    "The documentation happens to have some sample code for us to look at.¬†\n",
    "¬†\n",
    "```python\n",
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "\n",
    "def detectAndDisplay(frame):\n",
    "frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "frame_gray = cv.equalizeHist(frame_gray)\n",
    "\n",
    "#-- Detect faces\n",
    "faces = face_cascade.detectMultiScale(frame_gray)\n",
    "for (x,y,w,h) in faces:\n",
    "center = (x + w//2, y + h//2)\n",
    "frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)\n",
    "\n",
    "faceROI = frame_gray[y:y+h,x:x+w]\n",
    "#-- In each face, detect eyes\n",
    "eyes = eyes_cascade.detectMultiScale(faceROI)\n",
    "for (x2,y2,w2,h2) in eyes:\n",
    "eye_center = (x + x2 + w2//2, y + y2 + h2//2)\n",
    "radius = int(round((w2 + h2)*0.25))\n",
    "frame = cv.circle(frame, eye_center, radius, (255, 0, 0 ), 4)\n",
    "\n",
    "cv.imshow('Capture - Face detection', frame)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code for Cascade Classifier tutorial.')\n",
    "parser.add_argument('--face_cascade', help='Path to face cascade.', default='data/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "parser.add_argument('--eyes_cascade', help='Path to eyes cascade.', default='data/haarcascades/haarcascade_eye_tree_eyeglasses.xml')\n",
    "parser.add_argument('--camera', help='Camera divide number.', type=int, default=0)\n",
    "args = parser.parse_args()\n",
    "\n",
    "face_cascade_name = args.face_cascade\n",
    "eyes_cascade_name = args.eyes_cascade\n",
    "\n",
    "face_cascade = cv.CascadeClassifier()\n",
    "eyes_cascade = cv.CascadeClassifier()\n",
    "\n",
    "#-- 1. Load the cascades\n",
    "if not face_cascade.load(cv.samples.findFile(face_cascade_name)):\n",
    "print('--(!)Error loading face cascade')\n",
    "exit(0)\n",
    "if not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)):\n",
    "print('--(!)Error loading eyes cascade')\n",
    "exit(0)\n",
    "\n",
    "camera_device = args.camera\n",
    "#-- 2. Read the video stream\n",
    "cap = cv.VideoCapture(camera_device)\n",
    "if not cap.isOpened:\n",
    "print('--(!)Error opening video capture')\n",
    "exit(0)\n",
    "\n",
    "while True:\n",
    "ret, frame = cap.read()\n",
    "if frame is None:\n",
    "print('--(!) No captured frame -- Break!')\n",
    "break\n",
    "\n",
    "detectAndDisplay(frame)\n",
    "\n",
    "if cv.waitKey(10) == 27:\n",
    "break\n",
    "```\n",
    "¬†\n",
    "You'll notice that this is an example of the detection of the face and eyes.\n",
    "¬†\n",
    "The main thing you need to look at are the following lines.\n",
    "- `face_cascade = cv.CascadeClassifier()`\n",
    "- `faces = face_cascade.detectMultiScale(frame_gray)`\n",
    "- ```\n",
    "for (x,y,w,h) in faces:\n",
    "center = (x + w//2, y + h//2)\n",
    "frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "¬†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the basics of the Haar feature-based cascade classifier, we can apply it to detect stop signs. The provided code demonstrates face and eye detection using this technique, but we can modify it to detect stop signs instead.\n",
    "The key steps involved are:\n",
    "\n",
    "1. **Load the stop sign cascade classifier**: Instead of loading the face and eye cascades, we need to load the pre-trained stop sign cascade classifier. This can be done by replacing the lines:\n",
    "\n",
    "```python\n",
    "face_cascade = cv.CascadeClassifier()\n",
    "eyes_cascade = cv.CascadeClassifier()\n",
    "```\n",
    "to\n",
    "\n",
    "```python\n",
    "stop_sign_cascade = cv.CascadeClassifier('/home/user/stop_data.xml')\n",
    "```\n",
    "\n",
    "2. **Detect stop signs in the frame**: Instead of detecting faces and eyes, we need to detect stop signs using the loaded classifier. This can be done by replacing the line:\n",
    "\n",
    "```python\n",
    "faces = face_cascade.detectMultiScale(frame_gray)\n",
    "```\n",
    "to\n",
    "```python\n",
    "stop_signs = stop_sign_cascade.detectMultiScale(frame_gray)\n",
    "```\n",
    "\n",
    "3. **Draw bounding boxes around detected stop signs**: After detecting the stop signs, we can draw bounding boxes around them on the frame. Replace the loop:\n",
    "\n",
    "\n",
    "```python\n",
    "for (x,y,w,h) in faces:\n",
    "    center = (x + w//2, y + h//2)\n",
    "    frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4)\n",
    "    ```\n",
    "to\n",
    " ```python\n",
    "for (x,y,w,h) in stop_signs:\n",
    "    cv.rectangle(frame, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    object_detection.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('image_subscriber')\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.listener_callback,\n",
    "            10)\n",
    "        self.subscription  # prevent unused variable warning\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        # Load the stop sign classifier\n",
    "        self.stop_sign_classifier = cv2.CascadeClassifier('/home/user/stop_data.xml')\n",
    "        if self.stop_sign_classifier.empty():\n",
    "            self.get_logger().warning(\"Failed to load stop sign classifier\")\n",
    "\n",
    "    def listener_callback(self, data):\n",
    "        current_frame = self.bridge.imgmsg_to_cv2(data, desired_encoding='bgr8')\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect stop signs\n",
    "        if not self.stop_sign_classifier.empty():\n",
    "            stop_signs = self.stop_sign_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "            # Draw bounding boxes around the stop signs\n",
    "            for (x, y, w, h) in stop_signs:\n",
    "                cv2.rectangle(current_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", current_frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "    rclpy.spin(image_subscriber)\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rebuild and run our new package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "colcon build --packages-select object_detection\n",
    "source install/setup.bash\n",
    "ros2 run object_detection object_detection_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stop_detect.png\" style=\"width: 300px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Task 2</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Detect people üßç</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to successfully detect the stop sign using this classifier. This in combination with using the code included in the OpenCV documentation to train your own classifier, you can now use this approach to detect various kinds of objects in different situations.\n",
    "\n",
    "I suggest you try it in our turtlebot remote lab, where you can apply what we learned here and in the previous open classes.¬†\n",
    "\n",
    "However, using such a classifier is not perfect for every application.¬†\n",
    "\n",
    "The Haar feature-based cascade classifier approach for object detection has several downsides:\n",
    "\n",
    "1.  **Rigid Feature Design**: The Haar features are manually designed and can only capture a limited set of visual patterns. They may not be effective in detecting objects with more complex appearances or under varying lighting conditions.\n",
    "2.  **Slow Training Process**: Training the cascade classifier requires a lengthy process of selecting the best features using Adaboost, which can be computationally expensive, especially for larger datasets and complex objects.\n",
    "3.  **Lack of Generalization**: The trained classifier is specific to the object it was trained on and may not generalize well to other object classes or variations of the same object class.\n",
    "4.  **Limited Accuracy**: While this approach can achieve reasonable accuracy for specific object classes like faces, its performance may not be as high as more modern deep learning-based methods, especially for complex object detection tasks.\n",
    "5.  **Rigid Bounding Box Output**: The cascade classifier only outputs rigid bounding boxes around detected objects, which may not be suitable for applications that require more precise object segmentation or localization.\n",
    "\n",
    "It is with these contrains that we come to **single-stage and two-stage detectors.**\n",
    "\n",
    "For this OpenClass we will focus mostly on single-stage detectors, as they are generally more computationally efficient and simpler to implement. In this class, we will be using the oh-so-popular 'YOLO' (you only look once) model.\n",
    "\n",
    "![](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2018/11/yolo_design.jpg?lossy=2&strip=1&webp=1)\n",
    "\n",
    "YOLO (You Only Look Once) is a popular one-stage object detection algorithm that has gained widespread adoption due to its speed and accuracy. Unlike traditional object detection methods that involve region proposal and classification steps, YOLO treats object detection as a regression problem, allowing it to make predictions with a single evaluation of the deep neural network.\n",
    "\n",
    "We will be using the YOLOv3 model, all the files needed are included already. \n",
    "\n",
    "To use the model we will need to:\n",
    "\n",
    "1. Initialize and configure the YOLOv3 model then load the class labels.\n",
    "\n",
    "2. For each incoming image, preprocesses the image, run the YOLOv3 model to detect objects, and processes the detections.\n",
    "\n",
    "3. Apply non-max suppression to filter out redundant boxes and draw the resulting bounding boxes and labels on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize and configure the YOLOv3 model then load the class labels\n",
    "\n",
    "```python\n",
    "self.net = cv2.dnn.readNet(\"/home/user/yolov3.weights\", \"/home/user/yolov3.cfg\")\n",
    "self.layer_names = self.net.getLayerNames()\n",
    "unconnected_layers = self.net.getUnconnectedOutLayers()\n",
    "self.output_layers = [self.layer_names[i - 1] for i in unconnected_layers]\n",
    "\n",
    "with open(\"/home/user/coco.names\", \"r\") as f:\n",
    "    self.classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    ```\n",
    "1.  **Loading the Model**:\n",
    "\n",
    "    -   `self.net = cv2.dnn.readNet(\"/home/user/yolov3.weights\", \"/home/user/yolov3.cfg\")`\n",
    "        -   This line loads the YOLOv3 model from the specified weight file (`yolov3.weights`) and configuration file (`yolov3.cfg`).\n",
    "2.  **Getting Layer Names**:\n",
    "\n",
    "    -   `self.layer_names = self.net.getLayerNames()`\n",
    "        -   This retrieves the names of all the layers in the YOLOv3 network.\n",
    "3.  **Identifying Output Layers**:\n",
    "\n",
    "    -   `unconnected_layers = self.net.getUnconnectedOutLayers()`\n",
    "        -   This gets the indices of the unconnected (output) layers.\n",
    "    -   `self.output_layers = [self.layer_names[i - 1] for i in unconnected_layers]`\n",
    "        -   This creates a list of output layer names by mapping the indices to the corresponding names. Note that `i - 1` is used because the layer indices returned by `getUnconnectedOutLayers()` are 1-based, while Python lists are 0-based.\n",
    "4.  **Loading Class Labels**:\n",
    "\n",
    "    -   `with open(\"/home/user/coco.names\", \"r\") as f:`\n",
    "    -   `self.classes = [line.strip() for line in f.readlines()]`\n",
    "        -   This opens the `coco.names` file, reads the class names line by line, and stores them in a list. These class names correspond to the object classes that YOLOv3 can detect.\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. For each incoming image, preprocesses the image, run the YOLOv3 model to detect objects, and processes the detections\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(current_frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "self.net.setInput(blob)\n",
    "outs = self.net.forward(self.output_layers)\n",
    "\n",
    "```\n",
    "\n",
    "-   **Creating a Blob**:\n",
    "\n",
    "    -   `blob = cv2.dnn.blobFromImage(current_frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)`\n",
    "        -   This converts the input image (`current_frame`) into a blob suitable for YOLOv3.\n",
    "        -   Parameters:\n",
    "            -   `0.00392`: Scale factor (1/255), normalizes pixel values to [0, 1].\n",
    "            -   `(416, 416)`: Size to which the image is resized.\n",
    "            -   `(0, 0, 0)`: Mean subtraction values.\n",
    "            -   `True`: Indicates BGR->RGB conversion.\n",
    "            -   `crop=False`: No cropping is applied.-   **Setting the Input**:\n",
    "\n",
    "    -   `self.net.setInput(blob)`\n",
    "        -   This sets the blob as the input to the neural network.-   **Forward Pass**:\n",
    "\n",
    "    -   `outs = self.net.forward(self.output_layers)`\n",
    "        -   This performs a forward pass through the network to get the detections from the output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Detections\n",
    "\n",
    "```python\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "height, width, channels = current_frame.shape\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "```\n",
    "\n",
    "-   **Initialize Lists**:\n",
    "\n",
    "    -   `class_ids`, `confidences`, and `boxes` are initialized to store detection results.-   **Image Dimensions**:\n",
    "\n",
    "    -   `height, width, channels = current_frame.shape` gets the dimensions of the input image.-   **Process Each Detection**:\n",
    "\n",
    "    -   For each output layer (`out` in `outs`):\n",
    "        -   For each detection in the layer:\n",
    "            -   `scores = detection[5:]` extracts the confidence scores for all classes.\n",
    "            -   `class_id = np.argmax(scores)` identifies the class with the highest score.\n",
    "            -   `confidence = scores[class_id]` gets the confidence score for the identified class.\n",
    "            -   If `confidence > 0.5`, the detection is considered valid.\n",
    "            -   The detection's bounding box parameters (`center_x`, `center_y`, `w`, `h`) are scaled to the size of the original image.\n",
    "            -   The box's coordinates are computed and appended to `boxes`.\n",
    "            -   The confidence and class ID are also stored in their respective lists.\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drawing Bounding Boxes\n",
    "\n",
    "```python\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(self.classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        color = (0, 255, 0)\n",
    "        cv2.rectangle(current_frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(current_frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "```\n",
    "\n",
    "-   **Non-Max Suppression (NMS)**:\n",
    "\n",
    "    -   `indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)`\n",
    "        -   This applies non-max suppression to remove overlapping bounding boxes.\n",
    "        -   Parameters:\n",
    "            -   `boxes`: List of bounding boxes.\n",
    "            -   `confidences`: List of confidence scores.\n",
    "            -   `0.5`: Confidence threshold.\n",
    "            -   `0.4`: NMS threshold.-   **Drawing Boxes**:\n",
    "\n",
    "    -   For each box that passes NMS (`i in indexes`):\n",
    "        -   `x, y, w, h = boxes[i]` retrieves the box's coordinates.\n",
    "        -   `label = str(self.classes[class_ids[i]])` gets the label for the detected class.\n",
    "        -   `confidence = confidences[i]` gets the confidence score.\n",
    "        -   `color = (0, 255, 0)` sets the box color to green.\n",
    "        -   `cv2.rectangle(current_frame, (x, y), (x + w, y + h), color, 2)` draws the rectangle.\n",
    "        -   `cv2.putText(current_frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)` adds the label and confidence above the rectangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    object_detection.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "import numpy as np\n",
    "\n",
    "class ImageSubscriber(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('image_subscriber')\n",
    "        self.subscription = self.create_subscription(\n",
    "            Image,\n",
    "            '/camera/image_raw',\n",
    "            self.listener_callback,\n",
    "            10)\n",
    "        self.subscription  # prevent unused variable warning\n",
    "        self.bridge = CvBridge()\n",
    "\n",
    "        # Load YOLOv3 model\n",
    "        self.net = cv2.dnn.readNet(\"/home/user/yolov3.weights\", \"/home/user/yolov3.cfg\")\n",
    "        self.layer_names = self.net.getLayerNames()\n",
    "        unconnected_layers = self.net.getUnconnectedOutLayers()\n",
    "        self.output_layers = [self.layer_names[i - 1] for i in unconnected_layers]\n",
    "\n",
    "        with open(\"/home/user/coco.names\", \"r\") as f:\n",
    "            self.classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    def listener_callback(self, data):\n",
    "        current_frame = self.bridge.imgmsg_to_cv2(data, desired_encoding='bgr8')\n",
    "\n",
    "        # Perform object detection\n",
    "        blob = cv2.dnn.blobFromImage(current_frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "\n",
    "        # Process detections\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        height, width, channels = current_frame.shape\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Non-max suppression to remove overlapping bounding boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(self.classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "                color = (0, 255, 0)\n",
    "                cv2.rectangle(current_frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(current_frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", current_frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    image_subscriber = ImageSubscriber()\n",
    "    rclpy.spin(image_subscriber)\n",
    "    image_subscriber.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets rebuild and run our package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in a Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "colcon build --packages-select object_detection\n",
    "source install/setup.bash\n",
    "ros2 run object_detection object_detection_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/detecting_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll see that it was able to detect the stop sign ... and another stop sign ? We will come back to this. \n",
    "\n",
    "One thing you will want to see is if you turn back you'll see another thing that will be detected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/person.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because YOLO can be trained to detect several things at once, this particular model was trained on the COCO dataset.\n",
    "\n",
    "https://cocodataset.org/#home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge üö®\n",
    "\n",
    "Two stop signs ??? But there is only one !\n",
    "\n",
    "This is a very simple fix but you need to be **confident** in your programming skills !\n",
    "\n",
    "Good luck !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do you should get something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/fixed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HomeWork\n",
    "\n",
    "Right now, you'll notice the elephant in the room‚Äîwell,¬† more like the tree. Because our model does not seem to be able to detect this tree that we saw at the start.¬†\n",
    "\n",
    "This is a common situation if you are relying on detecting a very specific kind of object that may not be included in the dataset of the pretrained versions.¬†\n",
    "\n",
    "Try to use one of the methods taught in today's open class to be able to detect this tree, and let us know you did it!\n",
    "\n",
    "When you have completed it, take a screen recording and tag us on X (twitter) or facebook.¬†\n",
    "\n",
    "![](images/tree.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
